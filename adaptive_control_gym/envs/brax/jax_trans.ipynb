{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadrotor transportation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import struct\n",
    "from jax import numpy as jnp\n",
    "\n",
    "@struct.dataclass\n",
    "class EnvState:\n",
    "    y: float\n",
    "    z: float\n",
    "    theta: float\n",
    "    phi: float\n",
    "    y_dot: float\n",
    "    z_dot: float\n",
    "    theta_dot: float\n",
    "    phi_dot: float\n",
    "    last_thrust: float  # Only needed for rendering\n",
    "    last_tau: float  # Only needed for rendering\n",
    "    time: int\n",
    "    y_traj: jnp.ndarray\n",
    "    z_traj: jnp.ndarray\n",
    "    y_dot_traj: jnp.ndarray\n",
    "    z_dot_traj: jnp.ndarray\n",
    "    y_tar: float\n",
    "    z_tar: float\n",
    "    y_dot_tar: float\n",
    "    z_dot_tar: float\n",
    "    y_hook: float\n",
    "    z_hook: float\n",
    "    y_hook_dot: float\n",
    "    z_hook_dot: float\n",
    "    y_obj: float\n",
    "    z_obj: float\n",
    "    y_obj_dot: float\n",
    "    z_obj_dot: float\n",
    "    f_rope: float\n",
    "    f_rope_y: float\n",
    "    f_rope_z: float\n",
    "    l_rope: float\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class EnvParams:\n",
    "    max_speed: float = 8.0\n",
    "    max_torque: float = 0.012\n",
    "    max_thrust: float = 0.8\n",
    "    dt: float = 0.02\n",
    "    g: float = 9.81  # gravity\n",
    "    m: float = 0.03  # mass\n",
    "    I: float = 2.0e-5  # moment of inertia\n",
    "    mo: float = 0.005  # mass of the object attached to the rod\n",
    "    l: float = 0.3  # length of the rod\n",
    "    delta_yh: float = 0.03  # y displacement of the hook from the quadrotor center\n",
    "    delta_zh: float = -0.06  # z displacement of the hook from the quadrotor center\n",
    "    max_steps_in_episode: int = 300\n",
    "    rope_taut_therehold: float = 1e-4\n",
    "\n",
    "@struct.dataclass\n",
    "class Action:\n",
    "    thrust: float\n",
    "    tau: float\n",
    "\n",
    "def angle_normalize(x: float) -> float:\n",
    "    \"\"\"Normalize the angle - radians.\"\"\"\n",
    "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taut dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy.physics.mechanics import (\n",
    "    dynamicsymbols,\n",
    "    ReferenceFrame,\n",
    "    Point,\n",
    "    Particle,\n",
    "    RigidBody,\n",
    "    inertia,\n",
    ")\n",
    "from jax import numpy as jnp    \n",
    "\n",
    "# Define symbols\n",
    "t = sp.Symbol(\"t\")  # time\n",
    "m = sp.Symbol(\"m\", positive=True)  # mass of the quadrotor\n",
    "I = sp.Symbol(\"I\", positive=True)  # moment of inertia\n",
    "g = sp.Symbol(\"g\", positive=True)  # gravitational acceleration\n",
    "l = sp.Symbol(\"l\", positive=True)  # length of the rod\n",
    "mo = sp.Symbol(\"mo\", positive=True)  # mass of the object attached to the rod\n",
    "thrust = sp.Function(\"thrust\")(t)  # thrust force\n",
    "tau = sp.Function(\"tau\")(t)  # torque\n",
    "f_rope = sp.Symbol(\"f_rope\")  # force in the rope\n",
    "delta_yh = sp.Symbol(\"delta_yh\")  # y displacement of the hook from the quadrotor center\n",
    "delta_zh = sp.Symbol(\"delta_zh\")  # z displacement of the hook from the quadrotor center\n",
    "params = [m, I, g, l, mo, delta_yh, delta_zh]\n",
    "action = [thrust, tau]\n",
    "\n",
    "# Define state variables and their derivatives\n",
    "y, z, theta, phi = dynamicsymbols(\"y z theta phi\")\n",
    "y_dot, z_dot, theta_dot, phi_dot = sp.diff(y, t), sp.diff(z, t), sp.diff(theta, t), sp.diff(phi, t)\n",
    "y_ddot, z_ddot, theta_ddot, phi_ddot = sp.diff(y_dot, t), sp.diff(z_dot, t), sp.diff(theta_dot, t), sp.diff(phi_dot, t)\n",
    "y_dot_val, z_dot_val, theta_dot_val, phi_dot_val = sp.symbols(\"y_dot_val z_dot_val theta_dot_val phi_dot_val\")\n",
    "y_ddot_val, z_ddot_val, theta_ddot_val, phi_ddot_val = sp.symbols(\"y_ddot_val z_ddot_val theta_ddot_val phi_ddot_val\")\n",
    "states = [y, z, theta, phi, y_dot, z_dot, theta_dot, phi_dot]\n",
    "states_val = [y, z, theta, phi, y_dot_val, z_dot_val, theta_dot_val, phi_dot_val]\n",
    "states_dot = [y_ddot, z_ddot, theta_ddot, phi_ddot, f_rope]\n",
    "states_dot_val = [y_ddot_val, z_ddot_val, theta_ddot_val, phi_ddot_val, f_rope]\n",
    "\n",
    "# intermediate variables\n",
    "delta_yh_global = delta_yh * sp.cos(theta) - delta_zh * sp.sin(theta)\n",
    "delta_zh_global = delta_yh * sp.sin(theta) + delta_zh * sp.cos(theta)\n",
    "f_rope_y = f_rope * sp.sin(theta+phi)\n",
    "f_rope_z = -f_rope * sp.cos(theta+phi)\n",
    "y_hook = y + delta_yh_global\n",
    "z_hook = z + delta_zh_global\n",
    "y_hook_dot = sp.diff(y_hook, t)\n",
    "z_hook_dot = sp.diff(z_hook, t)\n",
    "y_obj = y_hook + l * sp.sin(theta+phi)\n",
    "z_obj = z_hook - l * sp.cos(theta+phi)\n",
    "y_obj_dot = sp.diff(y_obj, t)\n",
    "z_obj_dot = sp.diff(z_obj, t)\n",
    "y_obj_ddot = sp.diff(y_obj_dot, t)\n",
    "z_obj_ddot = sp.diff(z_obj_dot, t)\n",
    "obses = [y_obj, z_obj, y_obj_dot, z_obj_dot, y_obj_ddot, z_obj_ddot, f_rope_y, f_rope_z]\n",
    "\n",
    "# Define inertial reference frame\n",
    "N = ReferenceFrame(\"N\")\n",
    "N_origin = Point('N_origin')\n",
    "A = N.orientnew(\"A\", \"Axis\", [theta, N.x])\n",
    "B = A.orientnew(\"B\", \"Axis\", [phi, A.x])\n",
    "\n",
    "# Define point\n",
    "drone = Point(\"drone\")\n",
    "drone.set_pos(N_origin, y * N.y + z * N.z)\n",
    "hook = drone.locatenew(\"hook\", delta_yh * A.y + delta_zh * A.z)\n",
    "obj = hook.locatenew(\"obj\", -l * B.z)\n",
    "drone.set_vel(N, y_dot * N.y + z_dot * N.z)\n",
    "\n",
    "# Inertia\n",
    "inertia_quadrotor = inertia(N, I, 0, 0)\n",
    "quadrotor = RigidBody(\"quadrotor\", drone, A, m, (inertia_quadrotor, drone))\n",
    "obj_particle = Particle(\"obj_particle\", obj, mo)\n",
    "\n",
    "# Newton's law\n",
    "eq_quad_y = -thrust * sp.sin(theta) + f_rope_y - m * y_ddot\n",
    "eq_quad_z = thrust * sp.cos(theta) + f_rope_z - m * g - m * z_ddot\n",
    "eq_quad_theta = tau + delta_yh_global * f_rope_z - delta_zh_global * f_rope_y - I * theta_ddot\n",
    "eq_obj_y = -f_rope_y - mo * y_obj_ddot\n",
    "eq_obj_z = -f_rope_z - mo * g - mo * z_obj_ddot\n",
    "\n",
    "eqs = [eq_quad_y, eq_quad_z, eq_quad_theta, eq_obj_y, eq_obj_z]\n",
    "eqs = [eq.expand() for eq in eqs]\n",
    "eqs = [eq.subs([(states_dot[i], states_dot_val[i]) for i in range(len(states_dot))]) for eq in eqs]\n",
    "eqs = [eq.subs([(states[i], states_val[i]) for i in range(len(states))]) for eq in eqs]\n",
    "# Solve for the acceleration\n",
    "A_taut_dyn = sp.zeros(5, 5)\n",
    "b_taut_dyn = sp.zeros(5, 1)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        A_taut_dyn[i, j] = eqs[i].coeff(states_dot_val[j])\n",
    "    b_taut_dyn[i] = -eqs[i].subs([(states_dot_val[j], 0) for j in range(5)])\n",
    "# lambda A_taut_dyn\n",
    "A_taut_dyn_func = sp.lambdify(params + states_val + action, A_taut_dyn, \"jax\")\n",
    "b_taut_dyn_func = sp.lambdify(params + states_val + action, b_taut_dyn, \"jax\")\n",
    "\n",
    "# Solve for observation\n",
    "obs_eqs = [y_obj, z_obj, y_obj_dot, z_obj_dot, y_hook, z_hook, y_hook_dot, z_hook_dot, f_rope_y, f_rope_z]\n",
    "# replace states with states_val, states_dot with states_dot_val\n",
    "obs_eqs = [eq.subs([(states_dot[i], states_dot_val[i]) for i in range(len(states_dot))]) for eq in obs_eqs]\n",
    "obs_eqs = [eq.subs([(states[i], states_val[i]) for i in range(len(states))]) for eq in obs_eqs]\n",
    "# lambda obs_eqs\n",
    "obs_eqs_func = sp.lambdify(params + states_val + states_dot_val + action, obs_eqs, \"jax\")\n",
    "# lambda some other obs functions\n",
    "y_hook_func = sp.lambdify(params + states_val, obs_eqs[4], \"jax\")\n",
    "z_hook_func = sp.lambdify(params + states_val, obs_eqs[5], \"jax\")\n",
    "y_hook_dot_func = sp.lambdify(params + states_val, obs_eqs[6], \"jax\")\n",
    "z_hook_dot_func = sp.lambdify(params + states_val, obs_eqs[7], \"jax\")\n",
    "\n",
    "# dynamics (params, states) -> states_dot\n",
    "def taut_dynamics(env_params:EnvParams, env_state:EnvState, env_action:Action):\n",
    "    params = [env_params.m, env_params.I, env_params.g, env_params.l, env_params.mo, env_params.delta_yh, env_params.delta_zh]\n",
    "    states = [env_state.y, env_state.z, env_state.theta, env_state.phi, env_state.y_dot, env_state.z_dot, env_state.theta_dot, env_state.phi_dot]\n",
    "    action = [env_action.thrust, env_action.tau]\n",
    "    A = A_taut_dyn_func(*params, *states, *action)\n",
    "    b = b_taut_dyn_func(*params, *states, *action)\n",
    "    states_dot = jnp.linalg.solve(A, b).squeeze()\n",
    "    y_ddot, z_ddot, theta_ddot, phi_ddot, f_rope = states_dot\n",
    "\n",
    "    # Calculate updated state variables\n",
    "    new_y_dot = env_state.y_dot + y_ddot * env_params.dt\n",
    "    new_z_dot = env_state.z_dot + z_ddot * env_params.dt\n",
    "    new_theta_dot = env_state.theta_dot + theta_ddot * env_params.dt\n",
    "    new_phi_dot = env_state.phi_dot + phi_ddot * env_params.dt\n",
    "    new_y = env_state.y + new_y_dot * env_params.dt\n",
    "    new_z = env_state.z + new_z_dot * env_params.dt\n",
    "    new_theta = angle_normalize(env_state.theta + new_theta_dot * env_params.dt)\n",
    "    new_phi = angle_normalize(env_state.phi + new_phi_dot * env_params.dt)\n",
    "\n",
    "    # Update states list\n",
    "    states = [new_y, new_z, new_theta, new_phi, new_y_dot, new_z_dot, new_theta_dot, new_phi_dot]\n",
    "\n",
    "    # Compute other state variables\n",
    "    y_obj, z_obj, y_obj_dot, z_obj_dot, y_hook, z_hook, y_hook_dot, z_hook_dot, f_rope_y, f_rope_z = obs_eqs_func(*params, *states, *states_dot, *action)\n",
    "\n",
    "    # Update all state variables at once using replace()\n",
    "    env_state = env_state.replace(\n",
    "        y=new_y,\n",
    "        z=new_z,\n",
    "        theta=new_theta,\n",
    "        phi=new_phi,\n",
    "        y_dot=new_y_dot,\n",
    "        z_dot=new_z_dot,\n",
    "        theta_dot=new_theta_dot,\n",
    "        phi_dot=new_phi_dot,\n",
    "        y_obj=y_obj,\n",
    "        z_obj=z_obj,\n",
    "        y_obj_dot=y_obj_dot,\n",
    "        z_obj_dot=z_obj_dot,\n",
    "        y_hook=y_hook,\n",
    "        z_hook=z_hook,\n",
    "        y_hook_dot=y_hook_dot,\n",
    "        z_hook_dot=z_hook_dot,\n",
    "        f_rope_y=f_rope_y,\n",
    "        f_rope_z=f_rope_z,\n",
    "        f_rope=f_rope,\n",
    "        l_rope=env_params.l,\n",
    "        last_thrust=env_action.thrust,\n",
    "        last_tau=env_action.tau,\n",
    "        time=env_state.time + 1,\n",
    "        y_tar=env_state.y_traj[env_state.time],\n",
    "        z_tar=env_state.z_traj[env_state.time],\n",
    "        y_dot_tar=env_state.y_dot_traj[env_state.time],\n",
    "        z_dot_tar=env_state.z_dot_traj[env_state.time],\n",
    "    )\n",
    "\n",
    "    return env_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loose Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamics (params, states) -> states_dot\n",
    "def loose_dynamics(env_params:EnvParams, env_state:EnvState, env_action:Action):\n",
    "    params = [env_params.m, env_params.I, env_params.g, env_params.l, env_params.mo, env_params.delta_yh, env_params.delta_zh]\n",
    "    states = [env_state.y, env_state.z, env_state.theta, env_state.phi, env_state.y_dot, env_state.z_dot, env_state.theta_dot, env_state.phi_dot]\n",
    "    action = [env_action.thrust, env_action.tau]\n",
    "\n",
    "    y_ddot = -env_action.thrust * jnp.sin(env_state.theta) / env_params.m\n",
    "    z_ddot = env_action.thrust * jnp.cos(env_state.theta) / env_params.m - env_params.g\n",
    "    theta_ddot = env_action.tau / env_params.I\n",
    "\n",
    "    new_y_dot = env_state.y_dot + env_params.dt * y_ddot\n",
    "    new_z_dot = env_state.z_dot + env_params.dt * z_ddot\n",
    "    new_theta_dot = env_state.theta_dot + env_params.dt * theta_ddot\n",
    "    new_y = env_state.y + env_params.dt * new_y_dot\n",
    "    new_z = env_state.z + env_params.dt * new_z_dot\n",
    "    new_theta = angle_normalize(env_state.theta + env_params.dt * new_theta_dot)\n",
    "\n",
    "    states = [new_y, new_z, new_theta, env_state.phi, new_y_dot, new_z_dot, new_theta_dot, env_state.phi_dot]\n",
    "\n",
    "    y_hook = y_hook_func(*params, *states)\n",
    "    z_hook = z_hook_func(*params, *states)\n",
    "    y_hook_dot = y_hook_dot_func(*params, *states)\n",
    "    z_hook_dot = z_hook_dot_func(*params, *states)\n",
    "\n",
    "    new_y_obj_dot = env_state.y_obj_dot\n",
    "    new_z_obj_dot = env_state.z_obj_dot - env_params.g * env_params.dt\n",
    "    new_y_obj = env_state.y_obj + env_params.dt * new_y_obj_dot\n",
    "    new_z_obj = env_state.z_obj + env_params.dt * new_z_obj_dot\n",
    "\n",
    "    phi_th = -jnp.arctan2(y_hook - new_y_obj, z_hook - new_z_obj)\n",
    "    new_phi = angle_normalize(phi_th - new_theta)\n",
    "\n",
    "    y_obj2hook_dot = new_y_obj_dot - y_hook_dot\n",
    "    z_obj2hook_dot = new_z_obj_dot - z_hook_dot\n",
    "    phi_th_dot = y_obj2hook_dot * jnp.cos(phi_th) + z_obj2hook_dot * jnp.sin(phi_th)\n",
    "    new_phi_dot = phi_th_dot - new_theta_dot\n",
    "\n",
    "    new_l_rope = jnp.sqrt((y_hook - new_y_obj) ** 2 + (z_hook - new_z_obj) ** 2)\n",
    "\n",
    "    env_state = env_state.replace(\n",
    "        y=new_y,\n",
    "        z=new_z,\n",
    "        theta=new_theta,\n",
    "        phi=new_phi,\n",
    "        y_dot=new_y_dot,\n",
    "        z_dot=new_z_dot,\n",
    "        theta_dot=new_theta_dot,\n",
    "        phi_dot=new_phi_dot,\n",
    "        y_hook=y_hook,\n",
    "        z_hook=z_hook,\n",
    "        y_hook_dot=y_hook_dot,\n",
    "        z_hook_dot=z_hook_dot,\n",
    "        y_obj=new_y_obj,\n",
    "        z_obj=new_z_obj,\n",
    "        y_obj_dot=new_y_obj_dot,\n",
    "        z_obj_dot=new_z_obj_dot,\n",
    "        l_rope=new_l_rope,\n",
    "        f_rope=0.0,\n",
    "        f_rope_y=0.0,\n",
    "        f_rope_z=0.0,\n",
    "        last_thrust=env_action.thrust,\n",
    "        last_tau=env_action.tau,\n",
    "        time=env_state.time + 1,\n",
    "        y_tar=env_state.y_traj[env_state.time],\n",
    "        z_tar=env_state.z_traj[env_state.time],\n",
    "        y_dot_tar=env_state.y_dot_traj[env_state.time],\n",
    "        z_dot_tar=env_state.z_dot_traj[env_state.time],\n",
    "    )\n",
    "\n",
    "    return env_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define the variables\n",
    "I, m, l, mo, delta_yh, delta_zh, Imp, t = \\\n",
    "    sp.symbols('I m l mo delta_yh delta_zh Imp t')\n",
    "theta, phi, y, z, \\\n",
    "    loose_y_dot, loose_z_dot, loose_theta_dot, loose_y_obj_dot, loose_z_obj_dot, \\\n",
    "    taut_y_dot, taut_z_dot, taut_theta_dot, taut_phi_dot = \\\n",
    "    sp.symbols('theta phi y z \\\n",
    "                    loose_y_dot loose_z_dot loose_theta_dot loose_y_obj_dot loose_z_obj_dot \\\n",
    "                    taut_y_dot taut_z_dot taut_theta_dot taut_phi_dot')\n",
    "\n",
    "# Define velocities\n",
    "# get the linear velocity of the object in the world frame\n",
    "taut_xyz_obj_dot = sp.Matrix([0,taut_y_dot, taut_z_dot]) + \\\n",
    "                     sp.Matrix([taut_theta_dot,0,0]).cross(sp.Matrix([0,delta_yh * sp.cos(theta) - delta_zh * sp.sin(theta), delta_yh * sp.sin(theta) + delta_zh * sp.cos(theta)])) + \\\n",
    "                        sp.Matrix([taut_phi_dot+taut_theta_dot, 0,0]).cross(sp.Matrix([0,l * sp.sin(phi+theta), -l * sp.cos(phi+theta)])) \n",
    "taut_y_obj_dot, taut_z_obj_dot = taut_xyz_obj_dot[1], taut_xyz_obj_dot[2]\n",
    "\n",
    "# linear momentum balance for the quadrotor\n",
    "lin_quad_y = m * loose_y_dot + Imp * sp.sin(theta+phi) - m * taut_y_dot\n",
    "lin_quad_z = m * loose_z_dot - Imp * sp.cos(theta+phi) - m * taut_z_dot\n",
    "\n",
    "# linear momentum balance for the object\n",
    "lin_obj_y = sp.expand(mo * loose_y_obj_dot - Imp * sp.sin(theta+phi) - mo * taut_y_obj_dot)\n",
    "lin_obj_z = sp.expand(mo * loose_z_obj_dot + Imp * sp.cos(theta+phi) - mo * taut_z_obj_dot)\n",
    "\n",
    "# angular momentum balance for the quadrotor\n",
    "M = delta_yh * (- Imp * sp.cos(phi+theta)) - delta_zh * (Imp * sp.sin(phi+theta))\n",
    "ang_quad = I * loose_theta_dot + M - I * taut_theta_dot\n",
    "\n",
    "equations = [lin_quad_y, lin_quad_z, lin_obj_y, lin_obj_z, ang_quad]\n",
    "num_equations = len(equations)\n",
    "taut_state = [taut_y_dot, taut_z_dot, taut_theta_dot, taut_phi_dot, Imp]\n",
    "\n",
    "Atrans = sp.Matrix(num_equations, len(taut_state), lambda i, j: equations[i].coeff(taut_state[j]))\n",
    "btrans = -sp.Matrix(num_equations, 1, lambda i, j: equations[i].subs([(var, 0) for var in taut_state]))\n",
    "\n",
    "params = [I, m, l, mo, delta_yh, delta_zh]\n",
    "loose_state = [theta, phi, y, z, loose_y_dot, loose_z_dot, loose_theta_dot, loose_y_obj_dot, loose_z_obj_dot]\n",
    "\n",
    "# lambdify the matrix\n",
    "Atrans_func = sp.lambdify(params+loose_state, Atrans, modules='jax')\n",
    "btrans_func = sp.lambdify(params+loose_state, btrans, modules='jax')\n",
    "\n",
    "# Define the function to compute A^-1 (x-b)\n",
    "def loose2taut_transfer(env_params: EnvParams, loose_state: EnvState, loose2taut:bool):\n",
    "    params = [env_params.I, env_params.m, env_params.l, env_params.mo, env_params.delta_yh, env_params.delta_zh]\n",
    "    loose_state_values = [loose_state.theta, loose_state.phi, loose_state.y, loose_state.z, loose_state.y_dot, loose_state.z_dot, loose_state.theta_dot, loose_state.y_obj_dot, loose_state.z_obj_dot]\n",
    "    A_val = Atrans_func(*params, *loose_state_values)\n",
    "    b_val = btrans_func(*params, *loose_state_values)\n",
    "    taut_y_dot, taut_z_dot, taut_theta_dot, taut_phi_dot, Imp = jnp.linalg.solve(A_val, b_val).squeeze()\n",
    "\n",
    "    new_y_dot = taut_y_dot * loose2taut + loose_state.y_dot * (1 - loose2taut)\n",
    "    new_z_dot = taut_z_dot * loose2taut + loose_state.z_dot * (1 - loose2taut)\n",
    "    new_theta_dot = taut_theta_dot * loose2taut + loose_state.theta_dot * (1 - loose2taut)\n",
    "    new_phi_dot = taut_phi_dot * loose2taut + loose_state.phi_dot * (1 - loose2taut)\n",
    "    new_l_rope = env_params.l * loose2taut + loose_state.l_rope * (1 - loose2taut)\n",
    "\n",
    "    loose_state = loose_state.replace(\n",
    "        y_dot=new_y_dot,\n",
    "        z_dot=new_z_dot,\n",
    "        theta_dot=new_theta_dot,\n",
    "        phi_dot=new_phi_dot,\n",
    "        l_rope=new_l_rope\n",
    "    )\n",
    "\n",
    "    return loose_state\n",
    "\n",
    "def dynamic_transfer(env_params:EnvParams, loose_state:EnvState, taut_state:EnvState, old_loose_state:bool):\n",
    "    new_loose_state = loose_state.l_rope < (env_params.l - env_params.rope_taut_therehold)\n",
    "    taut2loose = (taut_state.f_rope < 0.0) & (~old_loose_state)\n",
    "    loose2taut = (~new_loose_state) & (old_loose_state)\n",
    "\n",
    "    # taut2loose dynamics\n",
    "    new_taut_l_rope = taut_state.l_rope - taut2loose * env_params.rope_taut_therehold * 2.0\n",
    "    taut_state = taut_state.replace(l_rope=new_taut_l_rope)\n",
    "\n",
    "    # loose2taut dynamics\n",
    "    loose_state = loose2taut_transfer(env_params, loose_state, loose2taut)\n",
    "\n",
    "    # use loose_state when old_loose_state is True, else use taut_state\n",
    "    new_state = {}\n",
    "    for k in loose_state.__dict__.keys():\n",
    "        new_state[k] = jnp.where(old_loose_state, loose_state.__dict__[k], taut_state.__dict__[k])\n",
    "\n",
    "    loose_state = loose_state.replace(**new_state)\n",
    "\n",
    "    return loose_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from gymnax.environments import environment, spaces\n",
    "from typing import Tuple, Optional\n",
    "import chex\n",
    "from flax import struct\n",
    "\n",
    "class Quad2D(environment.Environment):\n",
    "    \"\"\"\n",
    "    JAX Compatible version of Quad2D-v0 OpenAI gym environment. Source:\n",
    "    github.com/openai/gym/blob/master/gym/envs/classic_control/Quad2D.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.obs_shape = (24,)\n",
    "\n",
    "    @property\n",
    "    def default_params(self) -> EnvParams:\n",
    "        \"\"\"Default environment parameters for Quad2D-v0.\"\"\"\n",
    "        return EnvParams()\n",
    "    \n",
    "    def step_env(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: EnvState,\n",
    "        action: float,\n",
    "        params: EnvParams,\n",
    "    ) -> Tuple[chex.Array, EnvState, float, bool, dict]:\n",
    "        thrust = (action[0] + 1.0) / 2.0 * params.max_thrust\n",
    "        tau = action[1] * params.max_torque\n",
    "        reward = 1.0 - (\n",
    "            0.8\n",
    "            * jnp.sqrt(\n",
    "                (state.y_tar - state.y_obj) ** 2 + (state.z_tar - state.z_obj) ** 2\n",
    "            )  # close to center\n",
    "            + 0.03\n",
    "            * jnp.sqrt(\n",
    "                (state.y_dot_tar - state.y_obj_dot) ** 2\n",
    "                + (state.z_dot_tar - state.z_obj_dot) ** 2\n",
    "            )  # zero velocity\n",
    "        )\n",
    "        reward = reward.squeeze()\n",
    "        env_action = Action(thrust=thrust, tau=tau)\n",
    "\n",
    "        old_loose_state = state.l_rope < (params.l - params.rope_taut_therehold)\n",
    "        taut_state = taut_dynamics(params, state, env_action)\n",
    "        loose_state = loose_dynamics(params, state, env_action)\n",
    "        new_state = dynamic_transfer(params, loose_state, taut_state, old_loose_state)\n",
    "\n",
    "        done = self.is_terminal(state, params)\n",
    "        return (\n",
    "            lax.stop_gradient(self.get_obs(new_state)),\n",
    "            lax.stop_gradient(new_state),\n",
    "            reward,\n",
    "            done,\n",
    "            {\"discount\": self.discount(new_state, params)},\n",
    "        )\n",
    "\n",
    "    def reset_env(\n",
    "        self, key: chex.PRNGKey, params: EnvParams\n",
    "    ) -> Tuple[chex.Array, EnvState]:\n",
    "        \"\"\"Reset environment state by sampling theta, theta_dot.\"\"\"\n",
    "        # generate reference trajectory by adding a few sinusoids together\n",
    "        y_traj, z_traj, y_dot_traj, z_dot_traj = self.generate_traj(key)\n",
    "\n",
    "        high = jnp.array([1, 1, jnp.pi / 3])\n",
    "        y, z, theta = jax.random.uniform(key, shape=(3,), minval=-high, maxval=high)\n",
    "        y_hook = y + params.delta_yh * jnp.cos(theta) - params.delta_zh * jnp.sin(theta) \n",
    "        z_hook = z + params.delta_yh * jnp.sin(theta) + params.delta_zh * jnp.cos(theta)\n",
    "        state = EnvState(\n",
    "            y=y, z=z, theta=theta, \n",
    "            y_dot=0.0, z_dot=0.0, theta_dot=0.0,last_thrust=0.0,last_tau=0.0,time=0,\n",
    "            y_traj=y_traj,z_traj=z_traj,y_dot_traj=y_dot_traj,z_dot_traj=z_dot_traj,y_tar=y_traj[0],z_tar=z_traj[0],y_dot_tar=y_dot_traj[0],z_dot_tar=z_dot_traj[0],\n",
    "            phi=0.0,phi_dot=0.0,\n",
    "            y_hook=y_hook,z_hook=z_hook,y_hook_dot=0.0,z_hook_dot=0.0,\n",
    "            y_obj=y_hook + params.l * jnp.sin(theta), z_obj = z_hook - params.l * jnp.cos(theta), y_obj_dot=0.0, z_obj_dot=0.0,\n",
    "            f_rope=0.0, f_rope_y=0.0, f_rope_z=0.0,l_rope=params.l,\n",
    "        )\n",
    "        return self.get_obs(state), state\n",
    "\n",
    "    def generate_traj(self, key: chex.PRNGKey) -> chex.Array:\n",
    "        # get random attitude and phase\n",
    "        key_amp_y, key_phase_y, key_amp_z, key_phase_z = jax.random.split(key, 4)\n",
    "        rand_amp_y = jax.random.uniform(key_amp_y, shape=(2,), minval=-1.0, maxval=1.0)\n",
    "        rand_amp_z = jax.random.uniform(key_amp_z, shape=(2,), minval=-1.0, maxval=1.0)\n",
    "        rand_phase_y = jax.random.uniform(\n",
    "            key_phase_y, shape=(2,), minval=-jnp.pi, maxval=jnp.pi\n",
    "        )\n",
    "        rand_phase_z = jax.random.uniform(\n",
    "            key_phase_z, shape=(2,), minval=-jnp.pi, maxval=jnp.pi\n",
    "        )\n",
    "        # get trajectory\n",
    "        scale = 0.8\n",
    "        ts = jnp.arange(\n",
    "            0, self.default_params.max_steps_in_episode, self.default_params.dt\n",
    "        )  # NOTE: do not use params for jax limitation\n",
    "        w1 = 2 * jnp.pi * 0.25\n",
    "        w2 = 2 * jnp.pi * 0.5\n",
    "        y_traj = scale * rand_amp_y[0] * jnp.sin(\n",
    "            w1 * ts + rand_phase_y[0]\n",
    "        ) + scale * rand_amp_y[1] * jnp.sin(w2 * ts + rand_phase_y[1])\n",
    "        z_traj = scale * rand_amp_z[0] * jnp.sin(\n",
    "            w1 * ts + rand_phase_z[0]\n",
    "        ) + scale * rand_amp_z[1] * jnp.sin(w2 * ts + rand_phase_z[1])\n",
    "        y_dot_traj = scale * rand_amp_y[0] * w1 * jnp.cos(\n",
    "            w1 * ts + rand_phase_y[0]\n",
    "        ) + scale * rand_amp_y[1] * w2 * jnp.cos(w2 * ts + rand_phase_y[1])\n",
    "        z_dot_traj = scale * rand_amp_z[0] * w1 * jnp.cos(\n",
    "            w1 * ts + rand_phase_z[0]\n",
    "        ) + scale * rand_amp_z[1] * w2 * jnp.cos(w2 * ts + rand_phase_z[1])\n",
    "        return y_traj, z_traj, y_dot_traj, z_dot_traj\n",
    "\n",
    "    def get_obs(self, state: EnvState) -> chex.Array:\n",
    "        \"\"\"Return angle in polar coordinates and change.\"\"\"\n",
    "        return jnp.array(\n",
    "            [\n",
    "                state.y,\n",
    "                state.z,\n",
    "                state.theta,\n",
    "                state.y_dot / 4.0,\n",
    "                state.z_dot / 4.0,\n",
    "                state.theta_dot / 40.0,\n",
    "                state.y_tar,\n",
    "                state.z_tar,\n",
    "                state.y_dot_tar / 4.0,\n",
    "                state.z_dot_tar / 4.0,\n",
    "                state.y_tar - state.y,\n",
    "                state.z_tar - state.z,\n",
    "                (state.y_dot_tar - state.y_dot) / 4.0,\n",
    "                (state.z_dot_tar - state.z_dot) / 4.0,\n",
    "                state.phi,\n",
    "                state.phi_dot / 10.0,\n",
    "                state.y_obj,\n",
    "                state.z_obj,\n",
    "                state.y_obj_dot / 4.0,\n",
    "                state.z_obj_dot / 4.0,\n",
    "                state.y_hook,\n",
    "                state.z_hook,\n",
    "                state.y_hook_dot / 4.0,\n",
    "                state.z_hook_dot / 4.0,\n",
    "                # state.f_rope * 10.0,\n",
    "            ]\n",
    "        ).squeeze()\n",
    "\n",
    "    def is_terminal(self, state: EnvState, params: EnvParams) -> bool:\n",
    "        \"\"\"Check whether state is terminal.\"\"\"\n",
    "        # Check number of steps in episode termination condition\n",
    "        done = (\n",
    "            (state.time >= params.max_steps_in_episode)\n",
    "            | (jnp.abs(state.y) > 2.0)\n",
    "            | (jnp.abs(state.z) > 2.0)\n",
    "            | (jnp.abs(state.theta_dot) > 100.0)\n",
    "            | (jnp.abs(state.phi_dot) > 100.0)\n",
    "        )\n",
    "        return done\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Environment name.\"\"\"\n",
    "        return \"Quad2D-v1\"\n",
    "\n",
    "    @property\n",
    "    def num_actions(self) -> int:\n",
    "        \"\"\"Number of actions possible in environment.\"\"\"\n",
    "        return 2\n",
    "\n",
    "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
    "        \"\"\"Action space of the environment.\"\"\"\n",
    "        if params is None:\n",
    "            params = self.default_params\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(2,),\n",
    "            dtype=jnp.float32,\n",
    "        )\n",
    "\n",
    "    def observation_space(self, params: EnvParams) -> spaces.Box:\n",
    "        \"\"\"Observation space of the environment.\"\"\"\n",
    "        return spaces.Box(-1.0, 1.0, shape=(24,), dtype=jnp.float32)\n",
    "\n",
    "    def state_space(self, params: EnvParams) -> spaces.Dict:\n",
    "        \"\"\"State space of the environment.\"\"\"\n",
    "        return spaces.Dict(\n",
    "            {\n",
    "                \"y\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"z\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"y_dot\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"z_dot\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"theta\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"theta_dot\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"last_thrust\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"last_tau\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"time\": spaces.Discrete(params.max_steps_in_episode),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def angle_normalize(x: float) -> float:\n",
    "    \"\"\"Normalize the angle - radians.\"\"\"\n",
    "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def test_env(env: Quad2D, policy):\n",
    "    env_params = env.default_params\n",
    "    rng = jax.random.PRNGKey(1)\n",
    "\n",
    "    state_seq, obs_seq, reward_seq = [], [], []\n",
    "    rng, rng_reset = jax.random.split(rng)\n",
    "    obs, env_state = env.reset(rng_reset, env_params)\n",
    "    n_dones = 0\n",
    "    while True:\n",
    "        state_seq.append(env_state)\n",
    "        rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "        action = policy(obs, rng_act)\n",
    "        next_obs, next_env_state, reward, done, info = env.step(\n",
    "            rng_step, env_state, action, env_params\n",
    "        )\n",
    "        reward_seq.append(reward)\n",
    "        obs_seq.append(obs)\n",
    "        if done:\n",
    "            n_dones += 1\n",
    "        obs = next_obs\n",
    "        env_state = next_env_state\n",
    "        if n_dones >= 1:\n",
    "            break\n",
    "\n",
    "    # plot trajectory\n",
    "    def update_plot(frame_num):\n",
    "        plt.gca().clear()\n",
    "        plt.plot([s.y_obj for s in state_seq[0:frame_num+1]], [s.z_obj for s in state_seq[0:frame_num+1]], alpha=0.5)\n",
    "        plt.plot([s.y_tar for s in state_seq[0:frame_num+1]], [s.z_tar for s in state_seq[0:frame_num+1]], \"--\", alpha=0.2)\n",
    "        \n",
    "        start = max(0, frame_num)\n",
    "        for i in range(start, frame_num + 1):\n",
    "            num_steps = max(frame_num - start, 1)\n",
    "            alpha = 1 if i == frame_num else ((i-start) / num_steps * 0.1)\n",
    "            plt.arrow(\n",
    "                state_seq[i].y,\n",
    "                state_seq[i].z,\n",
    "                -0.1 * jnp.sin(state_seq[i].theta),\n",
    "                0.1 * jnp.cos(state_seq[i].theta),\n",
    "                width=0.01,\n",
    "                color=\"b\",\n",
    "                alpha=alpha,\n",
    "            )\n",
    "            # plot object as point\n",
    "            plt.plot(state_seq[i].y_obj, state_seq[i].z_obj, \"o\", color=\"b\", alpha=alpha)\n",
    "            # plot hook as cross\n",
    "            plt.plot(state_seq[i].y_hook, state_seq[i].z_hook, \"x\", color=\"r\", alpha=alpha)\n",
    "            plt.arrow(\n",
    "                state_seq[i].y_hook,\n",
    "                state_seq[i].z_hook,\n",
    "                state_seq[i].y_obj - state_seq[i].y_hook,\n",
    "                state_seq[i].z_obj - state_seq[i].z_hook,\n",
    "                width=0.01,\n",
    "                color=\"r\" if state_seq[i].l_rope > (env_params.l - env_params.rope_taut_therehold) else \"g\",\n",
    "                alpha=alpha,\n",
    "            )\n",
    "            # plot y_tar and z_tar with red dot\n",
    "            plt.plot(state_seq[i].y_tar, state_seq[i].z_tar, \"ro\", alpha=alpha)\n",
    "        plt.xlabel(\"y\")\n",
    "        plt.ylabel(\"z\")\n",
    "        plt.xlim([-2, 2])\n",
    "        plt.ylim([-2, 2])\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    anim = FuncAnimation(plt.gcf(), update_plot, frames=len(state_seq), interval=20)\n",
    "    anim.save(filename=\"../results/anim.gif\", writer=\"imagemagick\", fps=int(1.0/env_params.dt))\n",
    "\n",
    "    num_figs = len(state_seq[0].__dict__) + 2\n",
    "    time = [s.time * env_params.dt for s in state_seq]\n",
    "\n",
    "    # calculate number of rows needed\n",
    "    num_rows = int(jnp.ceil(num_figs / 6))\n",
    "\n",
    "    # create num_figs subplots\n",
    "    plt.subplots(num_rows, 6, figsize=(4 * 6, 2 * num_rows))\n",
    "\n",
    "    # plot reward\n",
    "    plt.subplot(num_rows, 6, 1)\n",
    "    plt.plot(time, reward_seq)\n",
    "    plt.ylabel(\"reward\")\n",
    "\n",
    "    # plot obs\n",
    "    plt.subplot(num_rows, 6, 2)\n",
    "    for i in range(len(obs_seq[0])):\n",
    "        plt.plot(time, [o[i] for o in obs_seq], label=f\"obs[{i}]\")\n",
    "    plt.ylabel(\"obs\")\n",
    "    plt.legend(fontsize=3)\n",
    "\n",
    "    # plot state\n",
    "    current_fig = 3\n",
    "    for i, (name, value) in enumerate(state_seq[0].__dict__.items()):\n",
    "        if name in [\"y_traj\", \"z_traj\", \"y_dot_traj\", \"z_dot_traj\", \"theta_traj\"]:\n",
    "            continue\n",
    "        current_fig += 1\n",
    "        plt.subplot(num_rows, 6, current_fig)\n",
    "        plt.plot(time, [getattr(s, name) for s in state_seq])\n",
    "        if name in [\"y\", \"z\", \"y_dot\", \"z_dot\"]:\n",
    "            plt.plot(time, [s.__dict__[name + \"_tar\"] for s in state_seq], \"--\")\n",
    "            plt.legend([\"actual\", \"target\"], fontsize=3)\n",
    "        plt.ylabel(name)\n",
    "\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/plot.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run PID policy to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Quad2D()\n",
    "\n",
    "random_policy = lambda obs, rng: env.action_space(env.default_params).sample(rng)\n",
    "\n",
    "def pid_policy(obs, rng):\n",
    "    y = obs[0]\n",
    "    z = obs[1]\n",
    "    theta = obs[2]\n",
    "    y_dot = obs[3] * 4.0\n",
    "    z_dot = obs[4] * 4.0\n",
    "    theta_dot = obs[5] * 40.0\n",
    "    y_tar = obs[6] * 0.0 # DEBUG\n",
    "    z_tar = obs[7] * 0.0\n",
    "    y_dot_tar = obs[8] * 4.0 * 0.0\n",
    "    z_dot_tar = obs[9] * 4.0 * 0.0\n",
    "\n",
    "    w0 = 10.0\n",
    "    zeta = 0.95\n",
    "    kp = env.default_params.m * (w0**2)\n",
    "    kd = env.default_params.m * 2.0 * zeta * w0\n",
    "    target_force_y = kp * (y_tar - y) + kd * (y_dot_tar - y_dot)\n",
    "    target_force_z = (\n",
    "        kp * (z_tar - z)\n",
    "        + kd * (z_dot_tar - z_dot)\n",
    "        + (env.default_params.m + env.default_params.mo) * env.default_params.g\n",
    "    )\n",
    "    thrust = -target_force_y * jnp.sin(theta) + target_force_z * jnp.cos(theta)\n",
    "    target_theta = -jnp.arctan2(target_force_y, target_force_z)\n",
    "\n",
    "    w0 = 30.0\n",
    "    zeta = 0.95\n",
    "    tau = env.default_params.I * (\n",
    "        (w0**2) * (target_theta - theta) + 2.0 * zeta * w0 * (0.0 - theta_dot)\n",
    "    )\n",
    "\n",
    "    # convert into action space\n",
    "    thrust_normed = jnp.clip(\n",
    "        thrust / env.default_params.max_thrust * 2.0 - 1.0, -1.0, 1.0\n",
    "    )\n",
    "    tau_normed = jnp.clip(tau / env.default_params.max_torque, -1.0, 1.0)\n",
    "    return jnp.array([thrust_normed, tau_normed])\n",
    "\n",
    "\n",
    "# with jax.disable_jit():\n",
    "test_env(env, policy=pid_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax.envs.wrappers import training\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param('log_std', nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env= Quad2D()\n",
    "    env_params = env.default_params \n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 2048,\n",
    "    \"NUM_STEPS\": 300,\n",
    "    \"TOTAL_TIMESTEPS\": 1.0e7,\n",
    "    \"UPDATE_EPOCHS\": 2,\n",
    "    \"NUM_MINIBATCHES\": 320,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "rng = jax.random.PRNGKey(42)\n",
    "t0 = time.time()\n",
    "train_jit = jax.jit(make_train(config))\n",
    "print(f\"jit time: {time.time() - t0:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "out = jax.block_until_ready(train_jit(rng))\n",
    "print(f\"train time: {time.time() - t0:.2f} s\")\n",
    "plt.plot(out[\"metrics\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
