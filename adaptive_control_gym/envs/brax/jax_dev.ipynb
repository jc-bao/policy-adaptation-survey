{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax experinment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "t = sp.Symbol('t')  # time\n",
    "m = sp.Symbol('m', positive=True)  # mass of the quadrotor\n",
    "I = sp.Symbol('I', positive=True)  # moment of inertia\n",
    "g = sp.Symbol('g', positive=True)  # gravitational acceleration\n",
    "thrust = sp.Function('thrust')(t)  # thrust force\n",
    "tau = sp.Function('tau')(t)  # torque\n",
    "\n",
    "# Define state variables and their derivatives\n",
    "x, y, theta = sp.symbols('x y theta', cls=sp.Function)\n",
    "x = x(t)\n",
    "y = y(t)\n",
    "theta = theta(t)\n",
    "\n",
    "x_dot = x.diff(t)\n",
    "y_dot = y.diff(t)\n",
    "theta_dot = theta.diff(t)\n",
    "\n",
    "x_ddot = x_dot.diff(t)\n",
    "y_ddot = y_dot.diff(t)\n",
    "theta_ddot = theta_dot.diff(t)\n",
    "\n",
    "# Equations of motion (Newton's second law and torque equation)\n",
    "eq_x = sp.Eq(x_ddot, thrust / m * sp.sin(theta))\n",
    "eq_y = sp.Eq(y_ddot, thrust / m * sp.cos(theta) - g)\n",
    "eq_theta = sp.Eq(I * theta_ddot, tau)\n",
    "\n",
    "# Solve for second-order derivatives\n",
    "sol = sp.solve((eq_x, eq_y, eq_theta), (x_ddot, y_ddot, theta_ddot))\n",
    "print(sol)\n",
    "\n",
    "quadrotor_dynamics = sp.lambdify((x, y, theta, x_dot, y_dot, theta_dot, thrust, tau, m, I, g), list(sol.values()), 'jax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Quad Rigid Rope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "t = sp.Symbol('t')  # time\n",
    "m = sp.Symbol('m', positive=True)  # mass of the quadrotor\n",
    "I = sp.Symbol('I', positive=True)  # moment of inertia\n",
    "g = sp.Symbol('g', positive=True)  # gravitational acceleration\n",
    "l = sp.Symbol('l', positive=True)  # length of the rod\n",
    "mo = sp.Symbol('mo', positive=True)  # mass of the object attached to the rod\n",
    "thrust = sp.Function('thrust')(t)  # thrust force\n",
    "tau = sp.Function('tau')(t)  # torque\n",
    "delta_xh = sp.Symbol('delta_xh')  # x displacement of the hook from the quadrotor center\n",
    "delta_yh = sp.Symbol('delta_yh')  # y displacement of the hook from the quadrotor center\n",
    "\n",
    "# Define state variables and their derivatives\n",
    "x, y, theta, phi = sp.symbols('x y theta phi', cls=sp.Function)\n",
    "x = x(t)\n",
    "y = y(t)\n",
    "theta = theta(t)\n",
    "phi = phi(t)\n",
    "\n",
    "x_dot = x.diff(t)\n",
    "y_dot = y.diff(t)\n",
    "theta_dot = theta.diff(t)\n",
    "phi_dot = phi.diff(t)\n",
    "\n",
    "# Define positions of the hook and the object attached to the rod\n",
    "x_hook = x + delta_xh * sp.cos(theta) - delta_yh * sp.sin(theta)\n",
    "y_hook = y + delta_xh * sp.sin(theta) + delta_yh * sp.cos(theta)\n",
    "\n",
    "x_obj = x_hook + l * sp.sin(phi)\n",
    "y_obj = y_hook - l * sp.cos(phi)\n",
    "\n",
    "x_obj_dot = x_obj.diff(t)\n",
    "y_obj_dot = y_obj.diff(t)\n",
    "\n",
    "# Define the Lagrangian\n",
    "T = 1/2 * m * (x_dot**2 + y_dot**2) + 1/2 * I * theta_dot**2 + 1/2 * mo * (x_obj_dot**2 + y_obj_dot**2)\n",
    "V = m * g * y + mo * g * y_obj\n",
    "L = T - V\n",
    "\n",
    "# Compute the Euler-Lagrange equations\n",
    "LE_x = sp.diff(L.diff(x_dot), t) - L.diff(x)\n",
    "LE_y = sp.diff(L.diff(y_dot), t) - L.diff(y)\n",
    "LE_theta = sp.diff(L.diff(theta_dot), t) - L.diff(theta)\n",
    "LE_phi = sp.diff(L.diff(phi_dot), t) - L.diff(phi)\n",
    "\n",
    "# simplify the function\n",
    "LE_x = sp.simplify(LE_x - thrust * sp.sin(theta))\n",
    "LE_y = sp.simplify(LE_y - thrust * sp.cos(theta) + m * g + mo * g)\n",
    "LE_theta = sp.simplify(LE_theta - tau)\n",
    "LE_phi = sp.simplify(LE_phi)\n",
    "\n",
    "# Solve for second-order derivatives\n",
    "sol = sp.solve((LE_x, LE_y, LE_theta, LE_phi), (x_dot.diff(t), y_dot.diff(t), theta_dot.diff(t), phi_dot.diff(t)), simplify=False) # NOTE disable simplify to make it faster. \n",
    "print(sol)\n",
    "\n",
    "quadrotor_dynamics = sp.lambdify((x, y, theta, phi, x_dot, y_dot, theta_dot, phi_dot, thrust, tau, m, I, g, l, mo, delta_xh, delta_yh), list(sol.values()), 'jax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy.physics.mechanics import dynamicsymbols, ReferenceFrame, Point, Particle, RigidBody, inertia, KanesMethod\n",
    "\n",
    "# Define symbols\n",
    "t = sp.Symbol('t')  # time\n",
    "m = sp.Symbol('m', positive=True)  # mass of the quadrotor\n",
    "I = sp.Symbol('I', positive=True)  # moment of inertia\n",
    "g = sp.Symbol('g', positive=True)  # gravitational acceleration\n",
    "l = sp.Symbol('l', positive=True)  # length of the rod\n",
    "mo = sp.Symbol('mo', positive=True)  # mass of the object attached to the rod\n",
    "thrust = sp.Function('thrust')(t)  # thrust force\n",
    "tau = sp.Function('tau')(t)  # torque\n",
    "delta_xh = sp.Symbol('delta_xh')  # x displacement of the hook from the quadrotor center\n",
    "delta_yh = sp.Symbol('delta_yh')  # y displacement of the hook from the quadrotor center\n",
    "\n",
    "# Define state variables and their derivatives\n",
    "x, y, theta, phi = dynamicsymbols('x y theta phi')\n",
    "x_dot, y_dot, theta_dot, phi_dot = dynamicsymbols('x_dot y_dot theta_dot phi_dot')\n",
    "\n",
    "N = ReferenceFrame('N')\n",
    "A = N.orientnew('A', 'Axis', [theta, N.z])\n",
    "B = A.orientnew('B', 'Axis', [phi, A.z])\n",
    "\n",
    "# Define positions of the hook and the object attached to the rod\n",
    "O = Point('O')\n",
    "O.set_pos(O, x * N.x + y * N.y)\n",
    "hook = O.locatenew('hook', delta_xh * A.x - delta_yh * A.y)\n",
    "obj = hook.locatenew('obj', l * B.x)\n",
    "\n",
    "# Define velocities\n",
    "O.set_vel(N, x_dot * N.x + y_dot * N.y)\n",
    "hook.v2pt_theory(O, N, A)\n",
    "obj.v2pt_theory(hook, N, B)\n",
    "\n",
    "# Kinematic Differential Equations (KDEs)\n",
    "kde = [x_dot - x.diff(t),\n",
    "       y_dot - y.diff(t),\n",
    "       theta_dot - theta.diff(t),\n",
    "       phi_dot - phi.diff(t)]\n",
    "\n",
    "# Inertia\n",
    "inertia_quadrotor = inertia(N, 0, 0, I)\n",
    "quadrotor = RigidBody('quadrotor', O, A, m, (inertia_quadrotor, O))\n",
    "obj_particle = Particle('obj_particle', obj, mo)\n",
    "\n",
    "# Forces\n",
    "forces = [(O, thrust * A.y),\n",
    "          (O, -m * g * N.y),\n",
    "          (obj, -mo * g * N.y),\n",
    "          (A, tau * N.z)]\n",
    "\n",
    "# Kane's equations\n",
    "kane = KanesMethod(N, q_ind=(x, y, theta, phi), u_ind=(x_dot, y_dot, theta_dot, phi_dot), kd_eqs=kde)\n",
    "fr, fr_star = kane.kanes_equations(loads=forces, bodies=[quadrotor, obj_particle])\n",
    "\n",
    "# Solve for second-order derivatives\n",
    "sol = sp.solve(fr + fr_star, (x_dot.diff(t), y_dot.diff(t), theta_dot.diff(t), phi_dot.diff(t)), simplify=False) # NOTE disable simplify to make it faster.\n",
    "print(sol)\n",
    "\n",
    "quadrotor_dynamics = sp.lambdify((x, y, theta, phi, x_dot, y_dot, theta_dot, phi_dot, thrust, tau, m, I, g, l, mo, delta_xh, delta_yh), list(sol.values()), 'jax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "t = sp.Symbol('t')  # time\n",
    "m = sp.Symbol('m', positive=True)  # mass of the quadrotor\n",
    "I = sp.Symbol('I', positive=True)  # moment of inertia\n",
    "g = sp.Symbol('g', positive=True)  # gravitational acceleration\n",
    "l = sp.Symbol('l', positive=True)  # length of the rod\n",
    "mo = sp.Symbol('mo', positive=True)  # mass of the object attached to the rod\n",
    "thrust = sp.Function('thrust')(t)  # thrust force\n",
    "tau = sp.Function('tau')(t)  # torque\n",
    "T = sp.Symbol('T')  # rope force\n",
    "delta_xh = sp.Symbol('delta_xh')  # x displacement of the hook from the quadrotor center\n",
    "delta_yh = sp.Symbol('delta_yh')  # y displacement of the hook from the quadrotor center\n",
    "\n",
    "# Define state variables and their derivatives\n",
    "x, y, theta, phi = sp.symbols('x y theta phi', cls=sp.Function)\n",
    "x = x(t)\n",
    "y = y(t)\n",
    "theta = theta(t)\n",
    "phi = phi(t)\n",
    "\n",
    "x_dot = x.diff(t)\n",
    "y_dot = y.diff(t)\n",
    "theta_dot = theta.diff(t)\n",
    "phi_dot = phi.diff(t)\n",
    "\n",
    "# Define positions of the hook and the object attached to the rod\n",
    "# Convert local hook displacement to global frame\n",
    "delta_xh_global = delta_xh * sp.cos(theta) - delta_yh * sp.sin(theta)\n",
    "delta_yh_global = delta_xh * sp.sin(theta) + delta_yh * sp.cos(theta)\n",
    "x_hook = x + delta_xh_global\n",
    "y_hook = y + delta_yh_global\n",
    "\n",
    "x_obj = x_hook + l * sp.sin(phi)\n",
    "y_obj = y_hook - l * sp.cos(phi)\n",
    "\n",
    "x_obj_dot = x_obj.diff(t)\n",
    "y_obj_dot = y_obj.diff(t)\n",
    "\n",
    "# Linear accelerations\n",
    "a_x_quad = x_dot.diff(t)\n",
    "a_y_quad = y_dot.diff(t)\n",
    "a_x_obj = x_obj_dot.diff(t)\n",
    "a_y_obj = y_obj_dot.diff(t)\n",
    "\n",
    "# Angular acceleration\n",
    "phi_ddot = phi_dot.diff(t)\n",
    "\n",
    "# Newton's second law for linear motion of quadrotor\n",
    "F_x_quad = thrust * sp.sin(theta) - m * a_x_quad - T * sp.sin(phi)\n",
    "F_y_quad = thrust * sp.cos(theta) - m * a_y_quad + m * g - T * sp.cos(phi)\n",
    "\n",
    "# Newton's second law for linear motion of object\n",
    "F_x_obj = mo * a_x_obj + T * sp.sin(phi)\n",
    "F_y_obj = mo * a_y_obj - mo * g + T * sp.cos(phi)\n",
    "\n",
    "# Euler's equations for angular motion of rod\n",
    "M_phi = delta_xh_global * (- T * sp.cos(phi)) - delta_yh_global * (- T * sp.sin(phi)) - I * theta_dot.diff(t)\n",
    "\n",
    "# Equations of motion\n",
    "eq1 = F_x_quad\n",
    "eq2 = F_y_quad\n",
    "eq3 = F_x_obj\n",
    "eq4 = F_y_obj\n",
    "eq5 = M_phi\n",
    "\n",
    "# Solve for second-order derivatives and rope tension\n",
    "sol_NE = sp.solve((eq1, eq2, eq3, eq4, eq5), (x_dot.diff(t), y_dot.diff(t), theta_dot.diff(t), phi_dot.diff(t), T))\n",
    "print(sol_NE)\n",
    "\n",
    "# Simplify the solutions\n",
    "sol_NE_simplified = {key: sp.simplify(value) for key, value in sol_NE.items()}\n",
    "print(sol_NE_simplified)\n",
    "\n",
    "# Create a function for the second-order derivatives and rope tension\n",
    "quadrotor_dynamics_NE = sp.lambdify((x, y, theta, phi, x_dot, y_dot, theta_dot, phi_dot, thrust, tau, m, I, g, l, mo, delta_xh, delta_yh), list(sol_NE_simplified.values()), 'jax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from gymnax.environments import environment, spaces\n",
    "from typing import Tuple, Optional\n",
    "import chex\n",
    "from flax import struct\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class EnvState:\n",
    "    x: float\n",
    "    y: float\n",
    "    theta: float\n",
    "    x_dot: float\n",
    "    y_dot: float\n",
    "    theta_dot: float\n",
    "    last_thrust: float  # Only needed for rendering\n",
    "    last_tau: float  # Only needed for rendering\n",
    "    time: int\n",
    "    x_traj: jnp.ndarray\n",
    "    y_traj: jnp.ndarray\n",
    "    x_dot_traj: jnp.ndarray\n",
    "    y_dot_traj: jnp.ndarray\n",
    "    x_tar: float\n",
    "    y_tar: float\n",
    "    x_dot_tar: float\n",
    "    y_dot_tar: float\n",
    "\n",
    "@struct.dataclass\n",
    "class EnvParams:\n",
    "    max_speed: float = 8.0\n",
    "    max_torque: float = 500.0\n",
    "    max_thrust: float = 20.0\n",
    "    dt: float = 0.02\n",
    "    g: float = 9.81  # gravity\n",
    "    m: float = 1.0  # mass\n",
    "    I: float = 1.0  # moment of inertia\n",
    "    max_steps_in_episode: int = 200\n",
    "\n",
    "\n",
    "class Quad2D(environment.Environment):\n",
    "    \"\"\"\n",
    "    JAX Compatible version of Quad2D-v0 OpenAI gym environment. Source:\n",
    "    github.com/openai/gym/blob/master/gym/envs/classic_control/Quad2D.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.obs_shape = (14,)\n",
    "\n",
    "    @property\n",
    "    def default_params(self) -> EnvParams:\n",
    "        \"\"\"Default environment parameters for Quad2D-v0.\"\"\"\n",
    "        return EnvParams()\n",
    "\n",
    "    def step_env(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: EnvState,\n",
    "        action: float,\n",
    "        params: EnvParams,\n",
    "    ) -> Tuple[chex.Array, EnvState, float, bool, dict]:\n",
    "        \"\"\"Integrate Quad2D ODE and return transition.\"\"\"\n",
    "        thrust = (action[0]+1.0)/2.0 * params.max_thrust\n",
    "        tau = action[1] * params.max_torque\n",
    "        reward = 1.0 - (\n",
    "            0.8 * jnp.sqrt((state.x_tar-state.x) ** 2 + (state.y_tar - state.y) ** 2) # close to center\n",
    "            + 0.03 * jnp.sqrt((state.x_dot_tar - state.x_dot) ** 2 + (state.y_dot_tar - state.y_dot) ** 2) # zero velocity          \n",
    "        )\n",
    "        reward = reward.squeeze()\n",
    "\n",
    "        s_dot = quadrotor_dynamics(\n",
    "            state.x, state.y, state.theta, state.x_dot, state.y_dot, state.theta_dot, \n",
    "            thrust, tau, \n",
    "            params.m, params.I, params.g)\n",
    "        \n",
    "        new_x_dot = state.x_dot + params.dt * s_dot[0]\n",
    "        new_y_dot = state.y_dot + params.dt * s_dot[1]\n",
    "        new_th_dot = state.theta_dot + params.dt * s_dot[2]\n",
    "        new_x = state.x + params.dt * new_x_dot\n",
    "        new_y = state.y + params.dt * new_y_dot\n",
    "        new_th = angle_normalize(state.theta + params.dt * new_th_dot)\n",
    "\n",
    "        # Update state dict and evaluate termination conditions\n",
    "        time = state.time + 1\n",
    "        state = EnvState(\n",
    "            x=new_x,\n",
    "            y=new_y,\n",
    "            x_dot=new_x_dot,\n",
    "            y_dot=new_y_dot,\n",
    "            theta=new_th,\n",
    "            theta_dot=new_th_dot,\n",
    "            last_thrust=thrust,\n",
    "            last_tau=tau,\n",
    "            time=time, \n",
    "            x_traj=state.x_traj,\n",
    "            y_traj=state.y_traj,\n",
    "            x_dot_traj=state.x_dot_traj,\n",
    "            y_dot_traj=state.y_dot_traj,\n",
    "            x_tar=state.x_traj[time], \n",
    "            y_tar=state.y_traj[time],\n",
    "            x_dot_tar=state.x_dot_traj[time],\n",
    "            y_dot_tar=state.y_dot_traj[time], \n",
    "        )\n",
    "        done = self.is_terminal(state, params)\n",
    "        return (\n",
    "            lax.stop_gradient(self.get_obs(state)),\n",
    "            lax.stop_gradient(state),\n",
    "            reward,\n",
    "            done,\n",
    "            {\"discount\": self.discount(state, params)},\n",
    "        )\n",
    "\n",
    "    def reset_env(\n",
    "        self, key: chex.PRNGKey, params: EnvParams\n",
    "    ) -> Tuple[chex.Array, EnvState]:\n",
    "        \"\"\"Reset environment state by sampling theta, theta_dot.\"\"\"\n",
    "        high = jnp.array([1, 1, jnp.pi/3])\n",
    "        rand_val = jax.random.uniform(key, shape=(3,), minval=-high, maxval=high)\n",
    "\n",
    "        # generate reference trajectory by adding a few sinusoids together\n",
    "        # get random attitude and phase\n",
    "        rand_amp_x = jax.random.uniform(key, shape=(2,), minval=-1.0, maxval=1.0)\n",
    "        rand_amp_y = jax.random.uniform(key, shape=(2,), minval=-1.0, maxval=1.0)\n",
    "        rand_phase_x = jax.random.uniform(key, shape=(2,), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        rand_phase_y = jax.random.uniform(key, shape=(2,), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        \n",
    "        # get trajectory\n",
    "        scale = 0.8\n",
    "        ts = jnp.arange(0, self.default_params.max_steps_in_episode, self.default_params.dt) # NOTE: do not use params for jax limitation\n",
    "        w1 = 2 * jnp.pi * 0.4\n",
    "        w2 = 2 * jnp.pi * 0.8\n",
    "        x_traj = scale * rand_amp_x[0] * jnp.sin(w1 * ts + rand_phase_x[0]) + scale * rand_amp_x[1] * jnp.sin(w2 * ts + rand_phase_x[1])\n",
    "        y_traj = scale * rand_amp_y[0] * jnp.sin(w1 * ts + rand_phase_y[0]) + scale * rand_amp_y[1] * jnp.sin(w2 * ts + rand_phase_y[1])\n",
    "        x_dot_traj = scale * rand_amp_x[0] * w1 * jnp.cos(w1 * ts + rand_phase_x[0]) + scale * rand_amp_x[1] * w2 * jnp.cos(w2 * ts + rand_phase_x[1])\n",
    "        y_dot_traj = scale * rand_amp_y[0] * w1 * jnp.cos(w1 * ts + rand_phase_y[0]) + scale * rand_amp_y[1] * w2 * jnp.cos(w2 * ts + rand_phase_y[1])\n",
    "        \n",
    "        state = EnvState(\n",
    "            x=rand_val[0], \n",
    "            y=rand_val[1],\n",
    "            theta=rand_val[2],\n",
    "            x_dot=0.0,\n",
    "            y_dot=0.0,\n",
    "            theta_dot=0.0,\n",
    "            last_thrust=0.0,\n",
    "            last_tau=0.0,\n",
    "            time=0,\n",
    "            x_traj=x_traj,\n",
    "            y_traj=y_traj,\n",
    "            x_dot_traj=x_dot_traj,\n",
    "            y_dot_traj=y_dot_traj,\n",
    "            x_tar=x_traj[0], \n",
    "            y_tar=y_traj[0],\n",
    "            x_dot_tar=x_dot_traj[0],\n",
    "            y_dot_tar=y_dot_traj[0],\n",
    "        )\n",
    "\n",
    "        return self.get_obs(state), state\n",
    "\n",
    "    def get_obs(self, state: EnvState) -> chex.Array:\n",
    "        \"\"\"Return angle in polar coordinates and change.\"\"\"\n",
    "        return jnp.array(\n",
    "            [\n",
    "                state.x, \n",
    "                state.y,\n",
    "                state.theta,\n",
    "                state.x_dot / 4.0,\n",
    "                state.y_dot / 4.0,\n",
    "                state.theta_dot / 40.0,\n",
    "                state.x_tar,\n",
    "                state.y_tar,\n",
    "                state.x_dot_tar / 4.0,\n",
    "                state.y_dot_tar / 4.0,\n",
    "                state.x_tar - state.x,\n",
    "                state.y_tar - state.y,\n",
    "                (state.x_dot_tar - state.x_dot) / 4.0,\n",
    "                (state.y_dot_tar - state.y_dot) / 4.0,\n",
    "            ]\n",
    "        ).squeeze()\n",
    "\n",
    "    def is_terminal(self, state: EnvState, params: EnvParams) -> bool:\n",
    "        \"\"\"Check whether state is terminal.\"\"\"\n",
    "        # Check number of steps in episode termination condition\n",
    "        done = (state.time >= params.max_steps_in_episode) | (jnp.abs(state.x) > 2.0) | (jnp.abs(state.y) > 2.0)\n",
    "        return done\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Environment name.\"\"\"\n",
    "        return \"Quad2D-v1\"\n",
    "\n",
    "    @property\n",
    "    def num_actions(self) -> int:\n",
    "        \"\"\"Number of actions possible in environment.\"\"\"\n",
    "        return 2\n",
    "\n",
    "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
    "        \"\"\"Action space of the environment.\"\"\"\n",
    "        if params is None:\n",
    "            params = self.default_params\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(2,),\n",
    "            dtype=jnp.float32,\n",
    "        )\n",
    "\n",
    "    def observation_space(self, params: EnvParams) -> spaces.Box:\n",
    "        \"\"\"Observation space of the environment.\"\"\"\n",
    "        high = jnp.array([1.0, 1.0, jnp.pi, 4.0, 4.0, 40.0, 1.0, 1.0, 4.0, 4.0, 1.0, 1.0, 4.0, 4.0], dtype=jnp.float32)\n",
    "        return spaces.Box(-high, high, shape=(14,), dtype=jnp.float32)\n",
    "\n",
    "    def state_space(self, params: EnvParams) -> spaces.Dict:\n",
    "        \"\"\"State space of the environment.\"\"\"\n",
    "        return spaces.Dict(\n",
    "            {\n",
    "                \"x\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"y\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"x_dot\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"y_dot\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"theta\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"theta_dot\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"last_thrust\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"last_tau\": spaces.Box(\n",
    "                    -jnp.finfo(jnp.float32).max,\n",
    "                    jnp.finfo(jnp.float32).max,\n",
    "                    (),\n",
    "                    jnp.float32,\n",
    "                ),\n",
    "                \"time\": spaces.Discrete(params.max_steps_in_episode),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def angle_normalize(x: float) -> float:\n",
    "    \"\"\"Normalize the angle - radians.\"\"\"\n",
    "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env:Quad2D, policy):\n",
    "    env_params = env.default_params\n",
    "    rng = jax.random.PRNGKey(2)\n",
    "\n",
    "    state_seq, obs_seq, reward_seq = [], [], []\n",
    "    rng, rng_reset = jax.random.split(rng)\n",
    "    obs, env_state = env.reset(rng_reset, env_params)\n",
    "    n_dones = 0\n",
    "    while True:\n",
    "        state_seq.append(env_state)\n",
    "        rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "        action = policy(obs, rng_act)\n",
    "        next_obs, next_env_state, reward, done, info = env.step(\n",
    "            rng_step, env_state, action, env_params\n",
    "        )\n",
    "        reward_seq.append(reward)\n",
    "        obs_seq.append(obs)\n",
    "        if done:\n",
    "            n_dones += 1\n",
    "        obs = next_obs\n",
    "        env_state = next_env_state\n",
    "        if n_dones >= 1:\n",
    "            break\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    num_figs = len(state_seq[0].__dict__)\n",
    "    time = [s.time * env_params.dt for s in state_seq]\n",
    "    # create num_figs subplots\n",
    "    plt.subplots(num_figs, 1, figsize=(4, 2 * num_figs))\n",
    "    # plot reward\n",
    "    plt.subplot(num_figs, 1, 1)\n",
    "    plt.plot(time, reward_seq)\n",
    "    plt.ylabel(\"reward\")\n",
    "    # plot obs\n",
    "    plt.subplot(num_figs, 1, 2)\n",
    "    for i in range(len(obs_seq[0])):\n",
    "        plt.plot(time, [o[i] for o in obs_seq], label=f\"obs{i}\")\n",
    "        plt.legend(fontsize=6)\n",
    "    # enumerate states's attributes (state is EnvParams dataclass)\n",
    "    fig_id = 3\n",
    "    for (k, v) in state_seq[0].__dict__.items():\n",
    "        if k in ['x_traj', 'y_traj', 'x_dot_traj', 'y_dot_traj', 'theta_traj']:\n",
    "            continue\n",
    "        plt.subplot(num_figs, 1, fig_id)\n",
    "        plt.plot(time, [s.__dict__[k] for s in state_seq])\n",
    "        if k in ['x', 'y', 'x_dot', 'y_dot']:\n",
    "            plt.plot(time, [s.__dict__[k + '_tar'] for s in state_seq], '--')\n",
    "            plt.legend(['actual', 'target'], fontsize=6)\n",
    "        plt.ylabel(k)\n",
    "        fig_id += 1\n",
    "\n",
    "env = Quad2D()\n",
    "random_policy = lambda obs, rng: env.action_space(env.default_params).sample(rng)\n",
    "def pid_policy(obs, rng):\n",
    "    x = obs[0]\n",
    "    y = obs[1]\n",
    "    theta = obs[2]\n",
    "    x_dot = obs[3] * 4.0\n",
    "    y_dot = obs[4] * 4.0\n",
    "    theta_dot = obs[5] * 40.0\n",
    "    x_tar = obs[6]\n",
    "    y_tar = obs[7]\n",
    "    x_dot_tar = obs[8] * 4.0\n",
    "    y_dot_tar = obs[9] * 4.0\n",
    "\n",
    "    w0 = 10.0\n",
    "    zeta = 0.9\n",
    "    kp = env.default_params.m * (w0**2)\n",
    "    kd = env.default_params.m * 2.0 * zeta * w0\n",
    "    target_force_x = kp * (x_tar - x) + kd * (x_dot_tar - x_dot)\n",
    "    target_force_y = kp * (y_tar - y) + kd * (y_dot_tar - y_dot) + env.default_params.m * env.default_params.g\n",
    "    thrust = target_force_x * jnp.sin(theta) + target_force_y * jnp.cos(theta)\n",
    "    target_theta = jnp.arctan2(target_force_x, target_force_y)\n",
    "\n",
    "    w0 = 30.0\n",
    "    zeta = 0.85\n",
    "    tau = env.default_params.I * ((w0**2) * (target_theta-theta) + 2.0 * zeta * w0 * (0.0-theta_dot))\n",
    "\n",
    "    # convert into action space\n",
    "    thrust_normed = jnp.clip(thrust/env.default_params.max_thrust * 2.0 - 1.0, -1.0, 1.0)\n",
    "    tau_normed = jnp.clip(tau/env.default_params.max_torque, -1.0, 1.0)\n",
    "    return jnp.array([thrust_normed, tau_normed])\n",
    "test_env(env, policy=pid_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax.envs.wrappers import training\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param('log_std', nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env= Quad2D()\n",
    "    env_params = env.default_params \n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 2048,\n",
    "    \"NUM_STEPS\": 200,\n",
    "    \"TOTAL_TIMESTEPS\": 2e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 320,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "rng = jax.random.PRNGKey(42)\n",
    "t0 = time.time()\n",
    "train_jit = jax.jit(make_train(config))\n",
    "print(f\"jit time: {time.time() - t0:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "out = jax.block_until_ready(train_jit(rng))\n",
    "print(f\"train time: {time.time() - t0:.2f} s\")\n",
    "plt.plot(out[\"metrics\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network params\n",
    "import pickle\n",
    "with open(\"../results/ppo_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out[\"runner_state\"][0].params, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(1)\n",
    "env = Quad2D()\n",
    "apply_fn = out['runner_state'][0].apply_fn\n",
    "params= out['runner_state'][0].params\n",
    "def policy(obs, rng):\n",
    "    return apply_fn(params, obs)[0].mean()\n",
    "env.reset(rng)\n",
    "# test policy\n",
    "test_env(env, policy=policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
